{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP60711 Lab Introduction\n",
    "\n",
    "The first week's lab will **not be assessed**, as the goal is for you to get set up and get used to the tools you'll need for the coursework.\n",
    "\n",
    "If you have this file open, it's assumed you have gone through the `comp60711_part2_guide.html` file to get set up. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Python\n",
    "\n",
    "Python is the main programming language used in data science/machine learning, due to the vast array of popular and useful libraries. One such library is [`scikit-learn`](https://scikit-learn.org/stable/index.html), which we will be using in the coursework. If in doubt, we _strongly encourage_ you to read through the [`scikit-learn`](https://scikit-learn.org/stable/index.html) documentation where there are lots of examples.\n",
    "\n",
    "## Q1.1\n",
    "For this question, we will be using `sklearn.datasets.load_diabetes` to load a dataset to experiment with.\n",
    "\n",
    "Below you will find short question/instructions and code comments to guide your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the imports that you will need\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "# Load the data and labels\n",
    "data, labels = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split this data (and labels) into a train and test set, using a 75:25 split (75% training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and fit the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "\n",
    "# Fit the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the `mean_squared_error` of the prediction on the test set? (If you get a value close to 2848, then you're on the right track!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions\n",
    "\n",
    "# Calculate the mean squared error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, wrap the above steps into a single for loop, where we take a different split of the data. Record the mean squared error across 10 runs, and then report the mean and standard deviation of these errors. You should expect to output a mean and standard deviation MSE values around 2967 and 217, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to hold results\n",
    "\n",
    "# Loop over 10 runs\n",
    "\n",
    "    # Split using a different seed each time\n",
    "\n",
    "    # Create the model\n",
    "\n",
    "    # Fit the model\n",
    "\n",
    "    # Get the predictions\n",
    "\n",
    "    # Calculate the mean squared error\n",
    "\n",
    "# Print the mean and standard deviation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will fit the model using only a single feature, so that we can plotting the resulting line. The code below selects the relevant feature for you. Partial code for some steps is given, you will need to add the relevant variables, and fill in the missing lines (as specified by the comments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load and select a single feature\n",
    "data, labels = load_diabetes(return_X_y=True)\n",
    "data = data[:, np.newaxis, 2]\n",
    "# Split the data and labels again into training and testing sets\n",
    "\n",
    "# Create the model\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "# Predict on the test set\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "# Add the x and y values for the data below\n",
    "ax.scatter(color=\"black\", label=\"Data\")\n",
    "# Add the x and y values for the regression line below\n",
    "ax.plot(color=\"red\", linewidth=4, label=\"Regression Line\")\n",
    "# Label axes\n",
    "ax.set_xlabel(\"BMI (standardized)\")\n",
    "ax.set_ylabel(\"Progression\")\n",
    "# Create the legend\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.2\n",
    "\n",
    "In this question, we will instead look at clustering, on the famous iris dataset.\n",
    "\n",
    "Visualizing the data can help us better understand it. For a dataset with 4 features, one way to do this could be to plot every 2D combination of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant imports\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "# To get the combinations of the 4 features\n",
    "combs = list(combinations(range(4), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to loop over the data to plot each combination of features. We want to compare both the real classes and the cluster assignments to visualize how they compare. For this, we will use marker styles and marker colour to differentiate between the two. To do that, we will use [`seaborn`](https://seaborn.pydata.org/index.html) as it makes this simple. We need to store our results into a `pandas` DataFrame to enable this.\n",
    "\n",
    "Below, we have provided code to load the data and the start of the for loop is provided. Finish the code using the comments as a guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of plots\n",
    "# As `len(combs)=6`, a 2x3 grid makes sense.\n",
    "fig, axes = plt.subplots(2,3, figsize=(15,10))\n",
    "# Load the data\n",
    "data, labels = load_iris(return_X_y=True)\n",
    "# The names for each feature\n",
    "feature_names = [\"sepal length\", \"sepal width\",\n",
    "                 \"petal length\", \"petal width\"]\n",
    "# For loop\n",
    "# This gives us a counter (i), an axes (ax),\n",
    "# and the indexes of the features (feature_pair)\n",
    "for i, (ax, feature_pair) in enumerate(zip(axes.flatten(), combs)):\n",
    "    # Select the relevant columns of the data\n",
    "\n",
    "    # Create the model (using 3 clusters)\n",
    "\n",
    "    # Fit the model\n",
    "\n",
    "    # Get the feature names for the feature pair\n",
    "    x_name, y_name = \n",
    "    # Create the dataframe of results\n",
    "    # You will need to fill in the relevant variables\n",
    "    df = pd.DataFrame({\n",
    "        \"predictions\":\n",
    "        \"truth\":\n",
    "        x_name: \n",
    "        y_name: \n",
    "    })\n",
    "    # Create the scatter plot\n",
    "    # Only create the legend for the final plot\n",
    "    if i == len(combs)-1:\n",
    "        sns.scatterplot(\n",
    "            x=x_name,\n",
    "            y=y_name,\n",
    "            data=df,\n",
    "            hue=\"truth\",\n",
    "            style=\"predictions\",\n",
    "            ax=ax,\n",
    "            legend=\"brief\",\n",
    "            palette=\"viridis\"\n",
    "        )\n",
    "    else:\n",
    "        sns.scatterplot(\n",
    "            x=x_name,\n",
    "            y=y_name,\n",
    "            data=df,\n",
    "            hue=\"truth\",\n",
    "            style=\"predictions\",\n",
    "            ax=ax,\n",
    "            legend=False,\n",
    "            palette=\"viridis\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the graphs to see how well the clusters match, adding any comments below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Weka\n",
    "\n",
    "There are many graphical tools that are seeing both increased utility and prevalence, so it is useful to also have some experience with using these. Therefore, in addition to Python we will also be using [Weka](https://www.cs.waikato.ac.nz/ml/weka/), which you'll need to install. When you first open Weka, you'll have options such as \"Explorer\" and \"Experimenter\", which opens separate interfaces (see the images below).\n",
    "\n",
    "![Weka Chooser](weka_chooser.png)\n",
    "\n",
    "## Q2.1\n",
    "\n",
    "Start by opening the \"Explorer\" part of Weka. Then, open `weather.nominal.arff`. For this dataset, we are deciding whether to play tennis or not based on past weather conditions.\n",
    "\n",
    "In the \"Classify\" tab, use the following learning schemes, with the default settings to analyse the weather data (in `weather.arff`): `ZeroR` (majority class), `OneR`, `NaiveBayesSimple`, `J48`. For each, select \"Use training set\", and then do the same again using the default \"Percentage Split\" of 66%. Look at the accuracy for each of the models. What is the cause of the difference?\n",
    "\n",
    "Which classifier are you more likely to trust when determining whether to play?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.1 Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.2 \n",
    "\n",
    "For this, we will use the \"Experimenter\" tab to show how Weka can set up larger, more complex experiments, while collecting performance statistics and testing the significance of the results.\n",
    "\n",
    "First, click \"New\" to start a new experiment. By default, 10-fold cross-validation should be selected, and the experiment should be repeated 10 times. Add the \"breast-cancer\" and \"segment-challenge\" datasets, and the J48 and ZeroR algorithms. Run the experiment (in the \"Run\" tab).\n",
    "\n",
    "Then, in the \"Analyse\" tabs, click \"Experiment\" and then \"Perform test\" to load the results from the experiment, and then perform testing on the results. \n",
    "\n",
    "J48 should achieve an accuracy of 95.71% on the \"segment-challenge\" datasets. Also, the asterisk (\"\\*\") shows that ZeroR performed significantly worse on the \"segment-challenge\" dataset. Add a screenshot of this table below, making sure it's clearly legible!\n",
    "\n",
    "Then, analyse the results again but this time showing the standard deviations, so that we can see how robust the reported accuracies are. Does this change your interpretation of the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.2 Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If there's time\n",
    "If you have extra time, we strongly encourage you to play around with both the Python code in Q1 and Weka, as familiarity with these (particularly Python) will be helpful for the coursework. Some example questions are shown below, but what is most important is gaining an understanding of the topics on this course, and the tools available to you for using and analysing these models.\n",
    "\n",
    "For example, how else could you visualize the different features of the iris dataset? A pairwise scatter plot? 3D plots?\n",
    "\n",
    "Do you get a lower mean squared error when using a polynomial regression? At what order does the model overfit?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
